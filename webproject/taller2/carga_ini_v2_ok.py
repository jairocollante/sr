# -*- coding: utf-8 -*-
"""Taller-2-v1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LDZEszxVKFQpYykSMeJi7HkMEp6Kmi0z
"""

# Commented out IPython magic to ensure Python compatibility.
from time import time
tiempo_inicial = time() 
import os
import pandas as  pd
import numpy as np
import pandas_profiling 
import matplotlib.pyplot as plt
import seaborn as sns
import math
import json

from sklearn.model_selection import train_test_split
from sklearn.feature_selection import chi2
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, precision_recall_fscore_support
from sklearn.model_selection import GridSearchCV
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import StandardScaler


#from surprise import Reader
#from surprise import Dataset
#from surprise.model_selection import train_test_split
#from surprise import KNNBasic
#from surprise import evaluate, print_perf
#from surprise import accuracy
import random
from sklearn.preprocessing import LabelEncoder
from collections import defaultdict
# %matplotlib inline

#Suprise
#!pip install surprise
from surprise import SVD
from surprise import Reader
from surprise import Dataset
from surprise import accuracy
from surprise.model_selection import GridSearchCV as Gridsearch_svd 
from surprise.model_selection import train_test_split
import dill as pickle

"""# Funciones"""

def missing_values_table(df): 
        mis_val = df.isnull().sum()
        mis_val_percent = 100 * df.isnull().sum()/len(df)
        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)
        mis_val_table_ren_columns = mis_val_table.rename(
        columns = {0 : 'Missing Values', 1 : '% of Total Values'})
        return mis_val_table_ren_columns 

def cargar_set_datos():
  global review_df, categories_df1
  review_df = pd.read_pickle("review_df_mean.ft")
  categories_df1 = pd.read_pickle("categories_df1_tfidf.ft")
  

  global businessPeruser 
  businessPeruser = defaultdict(set)
  with open('businessPeruser.pickle', 'rb') as handle:
    businessPeruser = pickle.load(handle)
  
  global business_df
  business_df = pd.read_pickle("business_ok.ft")

def caracteristicas_usuario(userid):
  df_temporal_usuario=review_df.loc[review_df.user_id==userid,['business_id','class']]
  df_temporal_usuario=df_temporal_usuario.merge(categories_df1, how='left', left_on='business_id', right_index=True)
  pesos_chi2, pval=chi2(df_temporal_usuario[features],df_temporal_usuario['class'])
  pesos_chi2=np.nan_to_num(pesos_chi2)
  pesos_chi2_mask=pesos_chi2>0
  return features[pesos_chi2_mask]
  
def modelo_usuario_simple(userid):
  features_usuario =caracteristicas_usuario(userid)
  if len(features_usuario) < 1:
    print("El usuario no tiene caracteristicas importante por chi2")
    return None
  df_temporal_usuario_train=review_df.loc[review_df.user_id==userid,['business_id','class']]
  df_temporal_usuario_train=df_temporal_usuario_train.merge(categories_df1[features_usuario], how='left', left_on='business_id', right_index=True)
  #normalizacion de set de datos

  std_scale = StandardScaler()
  X_train = df_temporal_usuario_train[features_usuario]
  y_adasyn = df_temporal_usuario_train['class']
  X_adasyn = std_scale.fit_transform(X_train)

  k=3


  ## Best Model
  knn_clasif=KNeighborsClassifier(k)
  knn_clasif.fit(X_adasyn, y_adasyn)
  del df_temporal_usuario_train
  return knn_clasif

 
def modelo_usuario(userid):
  features_usuario =caracteristicas_usuario(userid)
  if len(features_usuario) < 1:
    print("El usuario no tiene caracteristicas importante por chi2")
    return None
  
  df_temporal_usuario_train=review_df.loc[review_df.user_id==userid,['business_id','class']]
  df_temporal_usuario_train=df_temporal_usuario_train.merge(categories_df1[features_usuario], how='left', left_on='business_id', right_index=True)

  #Balanceo y normalizacion de set de datos
  std_scale = StandardScaler()
  X_train = df_temporal_usuario_train[features_usuario]
  y_train = df_temporal_usuario_train['class']

  adasyn =  SMOTE(random_state=None) #ADASYN(random_state=88)
  X_train_scaled = std_scale.fit_transform(X_train)
  X_adasyn, y_adasyn = adasyn.fit_resample(X_train_scaled, y_train)

  ### Best k
  KNeighborsClassifier()
  if len(df_temporal_usuario_train['class']) >= 200:
    param_grid = {'n_neighbors': [5,10,15,20,50,100]}
  else:
    param_grid = {'n_neighbors': [5,10,15,20,50]}
  

  gs = GridSearchCV(estimator = KNeighborsClassifier(),cv=5,param_grid=param_grid)
  resultados =gs.fit(X_adasyn, y_adasyn)
  k = resultados.best_params_['n_neighbors']
 
  ## Best Model
  knn_clasif=KNeighborsClassifier(k)
  knn_clasif.fit(X_adasyn, y_adasyn)
  del df_temporal_usuario_train
  return knn_clasif

 
def Jaccard(s1,s2):
  number = len(s1.intersection(s2))
  denom =  len(s1.union(s2))
  return number/denom

def mostSimilar_usuario(userid,n):
  similares=[]
  busines =  businessPeruser[userid]
  for u2 in  businessPeruser:
    if u2==userid:continue
    sim = Jaccard(busines,  businessPeruser[u2])
    similares.append((sim,u2))

  similares.sort(key=lambda t: t[0],reverse= True)
  return similares[:n]
  
def negocios_noin_userid(userid,n):
  usuarios_similares = mostSimilar_usuario(userid,n)
  lista_negocios_ = set()
  lista_no =[]
 
  for simi, userid_ in usuarios_similares:
    lista_negocios_.update(businessPeruser[userid_])
  
  for b in lista_negocios_:
    if b not in businessPeruser[userid]:
      lista_no.append(b)
  return lista_no
 
def features_other_business(userid, busines_):
  std_scale = StandardScaler()
  features_usuario =caracteristicas_usuario(userid)
  X_train = categories_df1.loc[busines_,features_usuario].reset_index(drop=True)  
  X_train_scaled = std_scale.fit_transform(X_train)
  return X_train_scaled
  
def content_user_recomendation(userid,k):
  #Encontar modelo de clasificacion userid
  lendatos = len(businessPeruser[userid])
  
  if(lendatos<=2):
    print("Usuario con insuficientes ratings para el modelo")
    return None

  if(lendatos<=50):
    if k <= 50:
      k=50
    modelo  = modelo_usuario_simple(userid)
  else:
    modelo = modelo_usuario(userid)

  if modelo is None:
    return None

  #lista de negocios no visitados por userid
  business_other = negocios_noin_userid(userid,k)

  if len(business_other) < 1:
    print("Usuario con insuficientes K users, aumente el k")
    return None

  

  #Features de negocios no vistados
  features_other = features_other_business(userid, business_other)
  
  #predicccion por contenido
  pred =modelo.predict(features_other)

  pred = pd.DataFrame(list(zip(pred, business_other)),columns =['class', 'business_id']) 
  pred = pred[pred['class']==True]['business_id']

  if len(pred) <1:
    print("El modelo del usuario en la predicciÃ³n de la clase es False, tiene pocos Ratings y la mayoria en False")
    return None

  return pred
  
def best_n_hybrid_recomendations(userid,n,k):
  #n recomendaciones, basado en consumo de los k mas similares usuarios por jaccard
  print(" INICIA ... modelo_content_udata_n")

  data = modelo_content_udata_n(userid,k)
  if data is None:
    print("no datos")
    return

  print(" INICIA ... modelo_svd_best_n")
  prediciones = modelo_svd_best_n(data)

  # usar data en lugar de review_svd_prueba
  userid_tx = data.loc[data.user_id ==userid]['userid'].iloc[0]
  user_predictions=list(filter(lambda x: x[0]==userid_tx,prediciones))
  user_predictions.sort(key=lambda x : x.est, reverse=True)
  user_predictions=user_predictions[0:10]

  #user_predictions Se convierte a dataframe 
  labels = ['businessid','estimation']
  df_predictions = pd.DataFrame.from_records(list(map(lambda x: (x.iid, x.est) , user_predictions)), columns=labels)
  a =df_predictions['businessid'].values


  #unique bisuniess
  bisnes =data[data['businessid'].isin(a)][['business_id']].business_id.unique()

  #cargar business recomendados usuario activo userid
  bisness =business_df[business_df.business_id.isin(bisnes)]
  bisness =business_df[['business_id','name','state', 'city', 'address','stars','categories']]

  print(bisness.shape)

  return bisness 

def modelo_content_udata_n(userid,n):
  #los n negocios no vistos por el usuario,  basado en las caracteristicas de los negocios visitados por el usuario
  pred  =content_user_recomendation(userid,n)
  if pred is None:
    return None
  
  #sin incluir usuario activo, se optinen los ratings dados a estas empresas
  review_svd =review_df[(review_df['business_id'].isin(pred))][['user_id','business_id','mean_by_business']]

 
  #para reducir dimencionalidad
  review_svd = pd.pivot_table(review_svd, index='user_id', columns='business_id', values='mean_by_business',fill_value=0)

  #se toman las empresas o negocios mas votados a partir de la media
  suma_svd = review_svd.sum()
  suma_svd = suma_svd.sort_values(ascending=False)

  #se reduce por columnas
  review_svd =  review_svd.loc[:,suma_svd [suma_svd >= suma_svd.mean()].index]

  #filtar por filas,  por usuarios los 100 mas activos
  #reducion por filas
  suma_svd = review_svd.sum(axis =1)
  suma_svd = suma_svd.sort_values(ascending=False)

  review_svd =review_svd.loc[suma_svd.nlargest(100).index,:]

  #se transforma a data set original
  review_svd =review_svd.stack().reset_index(name ='mean_by_business')

  ## adicionar los rantins usuario activo 
  review_svd =pd.concat([review_svd,review_df[review_df.user_id==userid][['user_id','business_id','mean_by_business']]],axis=0)

  ###numerizar id_user business_id
  #Discretizar valores 
  labelEncoder = LabelEncoder()
  review_svd['userid'] = labelEncoder.fit_transform(review_svd['user_id'])
  review_svd['businessid'] = labelEncoder.fit_transform(review_svd['business_id'])

  return review_svd

def modelo_svd_best_n(data):
  reader = Reader( rating_scale = ( 1, 5 ) )
                 # 'lr_all':[0.01,0.002,0.005],
                #'reg_all':[0.01,0.02,0.04],

  data = Dataset.load_from_df(data[ [ 'userid', 'businessid', 'mean_by_business' ] ], reader )
  param_grid = {'n_factors': [5, 20,50,100],

                'n_epochs':[100,200,300],
              }

  gs = Gridsearch_svd(SVD,param_grid,measures=['rmse'],cv=5,n_jobs =5)
  gs.fit(data)
  # combination of parameters that gave the best RMSE score
  k = gs.best_params['rmse']['n_factors']
  n_epochs =gs.best_params['rmse']['n_epochs']

  #Predictions with best parameters
 
  data_ = data.build_full_trainset()
  algo = SVD( n_factors = k, n_epochs = n_epochs)
  algo.fit(data_)
  prediciones = algo.test(data_.build_anti_testset())

  return prediciones



# Datos
cargar_set_datos()
features=categories_df1.columns
print(len(features))

tiempo_final = time() 
tiempo_ejecucion1 = tiempo_final - tiempo_inicial
print('El tiempo de carga Datos fue:',tiempo_ejecucion1)


tiempo_inicial = time() 
empresas =  best_n_hybrid_recomendations('lQGJcwX105k17081f6pulg',10,10)
empresas = empresas[['business_id','name','state', 'city', 'address','stars','categories']]
print(empresas)

tiempo_final = time() 
tiempo_ejecucion = tiempo_final - tiempo_inicial
print('El  tiempo de ejecucion primera consulta fue:',tiempo_ejecucion)
print("tiempo total carga datos y consulta :", tiempo_ejecucion1+tiempo_ejecucion)

tiempo_inicial = time() 
empresas =  best_n_hybrid_recomendations('zzsmdXHUFBYuKUtPbXWjRA',10,150) 
empresas = empresas[['business_id','name','state', 'city', 'address','stars','categories']]
print(empresas)
tiempo_final = time() 
tiempo_ejecucion = tiempo_final - tiempo_inicial
print('El  tiempo segunda ejecucion, para usuario con menos ratings es ',tiempo_ejecucion)
